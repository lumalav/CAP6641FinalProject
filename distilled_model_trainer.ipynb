{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "kshot = '16-100'\n",
    "teacher_id = \"clean-model\\\\checkpoint-24-epoch-6\"\n",
    "student_id = \"distilroberta-base\"\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_id, use_fast=False)\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_id, use_fast=False)\n",
    "\n",
    "# sample input\n",
    "sample = \"This is a basic example, with different words to test.\"\n",
    "\n",
    "# assert results\n",
    "assert teacher_tokenizer(sample) == student_tokenizer(sample), \"Tokenizers haven't created the same output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['0', '1'], id=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pre-processing and tokenization\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def load_data(fileName):\n",
    "    df = pd.read_csv(f'BadPrompt\\\\data\\\\k-shot\\\\subj\\\\{kshot}\\\\{fileName}')\n",
    "    df['idx'] = range(1, len(df) + 1)\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.columns = ['label', 'sentence', 'idx']\n",
    "    df = df.reindex(columns=['sentence', 'label', 'idx'])\n",
    "    ds = Dataset.from_pandas(df)\n",
    "    ds = ds.class_encode_column(\"label\")\n",
    "    return ds\n",
    "\n",
    "def process(examples):\n",
    "    tokenized_inputs = teacher_tokenizer(\n",
    "        examples[\"sentence\"], truncation=True, padding=True\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "dataset = DatasetDict()\n",
    "\n",
    "dataset['train'] = load_data('train.csv')\n",
    "dataset['validation'] = load_data('dev.csv')\n",
    "dataset['test'] = load_data('test.csv')\n",
    "\n",
    "tokenized_datasets = dataset.map(process, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\",\"labels\")\n",
    "\n",
    "tokenized_datasets[\"train\"].features[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distill class\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "        \n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        # place teacher on same device as student\n",
    "        self._move_model_to_device(self.teacher,self.model.device)\n",
    "        self.teacher.eval()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # compute student output\n",
    "        outputs_student = model(**inputs)\n",
    "        student_loss=outputs_student.loss\n",
    "        # compute teacher output\n",
    "        with torch.no_grad():\n",
    "          outputs_teacher = self.teacher(**inputs)\n",
    "        \n",
    "        # assert size\n",
    "        assert outputs_student.logits.size() == outputs_teacher.logits.size()\n",
    "        \n",
    "        # Soften probabilities and compute distillation loss\n",
    "        loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        loss_logits = (loss_function(\n",
    "            F.log_softmax(outputs_student.logits / self.args.temperature, dim=-1),\n",
    "            F.softmax(outputs_teacher.logits / self.args.temperature, dim=-1)) * (self.args.temperature ** 2))\n",
    "        # Return weighted student loss\n",
    "        loss = self.args.alpha * student_loss + (1. - self.args.alpha) * loss_logits\n",
    "        return (loss, outputs_student) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter definition, model loading\n",
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "output_dir = f'distilled-model\\\\distilroberta-subj-{kshot}'\n",
    "# create label2id, id2label dicts for nice outputs for the model\n",
    "labels = tokenized_datasets[\"train\"].features[\"labels\"].names\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "# define training args\n",
    "training_args = DistillationTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    fp16=True,\n",
    "    learning_rate=6e-5,\n",
    "    seed=33,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{output_dir}\\\\logs\",\n",
    "    logging_strategy=\"epoch\", # to get more information to TB\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"tensorboard\",\n",
    "    # distilation parameters\n",
    "    alpha=0.5,\n",
    "    temperature=4.0\n",
    "    )\n",
    "\n",
    "# define data_collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=teacher_tokenizer)\n",
    "\n",
    "# define model\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_id,\n",
    "    num_labels=num_labels, \n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# define student model\n",
    "student_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    student_id,\n",
    "    num_labels=num_labels, \n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# define metrics and metrics function\n",
    "accuracy_metric = evaluate.load( \"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    return {\n",
    "        \"accuracy\": acc[\"accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "trainer = DistillationTrainer(\n",
    "    student_model,\n",
    "    training_args,\n",
    "    teacher_model=teacher_model,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=teacher_tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.use_cuda_amp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\llavieri\\.conda\\envs\\project6641\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  7%|▋         | 1/15 [00:00<00:04,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1465, 'learning_rate': 5.6e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 1/15 [00:00<00:04,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2808144092559814, 'eval_accuracy': 0.5161290322580645, 'eval_runtime': 0.0835, 'eval_samples_per_second': 371.044, 'eval_steps_per_second': 11.969, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:01<00:14,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1048, 'learning_rate': 5.2000000000000004e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|█▎        | 2/15 [00:02<00:14,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2565524578094482, 'eval_accuracy': 0.5161290322580645, 'eval_runtime': 0.0846, 'eval_samples_per_second': 366.452, 'eval_steps_per_second': 11.821, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:03<00:16,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0442, 'learning_rate': 4.8e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 3/15 [00:03<00:16,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.217442274093628, 'eval_accuracy': 0.5161290322580645, 'eval_runtime': 0.0855, 'eval_samples_per_second': 362.417, 'eval_steps_per_second': 11.691, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:05<00:17,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0203, 'learning_rate': 4.4e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 27%|██▋       | 4/15 [00:05<00:17,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1455793380737305, 'eval_accuracy': 0.5161290322580645, 'eval_runtime': 0.086, 'eval_samples_per_second': 360.455, 'eval_steps_per_second': 11.628, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [00:07<00:16,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9184, 'learning_rate': 3.9999999999999996e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 5/15 [00:07<00:16,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.010833740234375, 'eval_accuracy': 0.6451612903225806, 'eval_runtime': 0.0825, 'eval_samples_per_second': 375.593, 'eval_steps_per_second': 12.116, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:09<00:15,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7491, 'learning_rate': 3.6e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 6/15 [00:09<00:15,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7874598503112793, 'eval_accuracy': 0.9354838709677419, 'eval_runtime': 0.0836, 'eval_samples_per_second': 371.003, 'eval_steps_per_second': 11.968, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:10<00:13,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4591, 'learning_rate': 3.2e-05, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 47%|████▋     | 7/15 [00:10<00:13,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5193419456481934, 'eval_accuracy': 0.9354838709677419, 'eval_runtime': 0.082, 'eval_samples_per_second': 378.036, 'eval_steps_per_second': 12.195, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [00:12<00:12,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2157, 'learning_rate': 2.8e-05, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 53%|█████▎    | 8/15 [00:12<00:12,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2998932600021362, 'eval_accuracy': 0.9354838709677419, 'eval_runtime': 0.0825, 'eval_samples_per_second': 375.711, 'eval_steps_per_second': 12.12, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [00:14<00:10,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0469, 'learning_rate': 2.4e-05, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████    | 9/15 [00:14<00:10,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1088396310806274, 'eval_accuracy': 0.9354838709677419, 'eval_runtime': 0.0815, 'eval_samples_per_second': 380.29, 'eval_steps_per_second': 12.267, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [00:16<00:08,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.882, 'learning_rate': 1.9999999999999998e-05, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████▋   | 10/15 [00:16<00:08,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9221992492675781, 'eval_accuracy': 0.9354838709677419, 'eval_runtime': 0.083, 'eval_samples_per_second': 373.384, 'eval_steps_per_second': 12.045, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [00:17<00:07,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7255, 'learning_rate': 1.6e-05, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 73%|███████▎  | 11/15 [00:18<00:07,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7745137810707092, 'eval_accuracy': 0.9354838709677419, 'eval_runtime': 0.0841, 'eval_samples_per_second': 368.736, 'eval_steps_per_second': 11.895, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [00:19<00:05,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6339, 'learning_rate': 1.2e-05, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████  | 12/15 [00:19<00:05,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6806623935699463, 'eval_accuracy': 0.9354838709677419, 'eval_runtime': 0.0837, 'eval_samples_per_second': 370.358, 'eval_steps_per_second': 11.947, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [00:21<00:03,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.578, 'learning_rate': 8e-06, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 87%|████████▋ | 13/15 [00:21<00:03,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6051871180534363, 'eval_accuracy': 0.9354838709677419, 'eval_runtime': 0.082, 'eval_samples_per_second': 378.017, 'eval_steps_per_second': 12.194, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [00:23<00:01,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5176, 'learning_rate': 4e-06, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 93%|█████████▎| 14/15 [00:23<00:01,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5533555746078491, 'eval_accuracy': 0.9354838709677419, 'eval_runtime': 0.0875, 'eval_samples_per_second': 354.193, 'eval_steps_per_second': 11.426, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:25<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4671, 'learning_rate': 0.0, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 15/15 [00:25<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5277326703071594, 'eval_accuracy': 0.9354838709677419, 'eval_runtime': 0.083, 'eval_samples_per_second': 373.483, 'eval_steps_per_second': 12.048, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:26<00:00,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 26.7841, 'train_samples_per_second': 17.361, 'train_steps_per_second': 0.56, 'train_loss': 1.3006048560142518, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=1.3006048560142518, metrics={'train_runtime': 26.7841, 'train_samples_per_second': 17.361, 'train_steps_per_second': 0.56, 'train_loss': 1.3006048560142518, 'epoch': 15.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap6641project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
